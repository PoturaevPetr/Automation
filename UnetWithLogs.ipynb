{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbecd43",
   "metadata": {
    "cellId": "00jvi2ioh9uzt27ogktsm3n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n",
    "from keras.layers import  Dropout, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import backend as K\n",
    "# from keras.utils import plot_model\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from random import shuffle, randint, getrandbits\n",
    "tf.device('/physical_device:GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8c350",
   "metadata": {
    "cellId": "gj27n8jemvb4dw0cmrwwof"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true[:,:,:,0]\n",
    "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee587f7",
   "metadata": {
    "cellId": "7g1236kdn32hd8kgqr4o6t"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def unet(sz = (256, 256, 3)):\n",
    "    x = Input(sz)\n",
    "    inputs = x\n",
    "  \n",
    "  #down sampling \n",
    "    f = 8\n",
    "    layers = []\n",
    "  \n",
    "    for i in range(0, 6):\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "        layers.append(x)\n",
    "        x = MaxPooling2D() (x)\n",
    "        f = f*2\n",
    "    ff2 = 64 \n",
    "  \n",
    "  #bottleneck \n",
    "    j = len(layers) - 1\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
    "    x = Concatenate(axis=3)([x, layers[j]])\n",
    "    j = j -1 \n",
    "  \n",
    "  #upsampling \n",
    "    for i in range(0, 5):\n",
    "        ff2 = ff2//2\n",
    "        f = f // 2 \n",
    "        x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "        x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
    "        x = Concatenate(axis=3)([x, layers[j]])\n",
    "        j = j -1 \n",
    "    \n",
    "  \n",
    "  #classification \n",
    "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid') (x)\n",
    "  \n",
    "  #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f87c46",
   "metadata": {
    "cellId": "1edqyqezri81weg0u2670v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ce11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_dataset = \"datasets/segm_dataset_learn\"\n",
    "#folder_chkpt = \"segm_dataset_learn\"\n",
    "datasets_name = \"dataset_grain_0.1\"\n",
    "path_dataset = f\"datasets/Grain/{datasets_name}\"\n",
    "folder_chkpt = f\"Segments/{datasets_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9be40e",
   "metadata": {
    "cellId": "0cr4tyvp133liyu2qkqzl8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def image_generator(files, batch_size = 32, sz = (256, 256)):\n",
    "  \n",
    "  while True: \n",
    "    \n",
    "    #extract a random batch \n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs and outputs \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    \n",
    "    for f in batch:\n",
    "\n",
    "        #get the masks. Note that masks are png files \n",
    "        mask = mask2 = Image.open(f'{path_dataset}/masks/{f}')\n",
    "        mask = np.array(mask.resize(sz))[:,:,0]\n",
    "        mask2 = np.array(mask2)[:,:,0]\n",
    "\n",
    "        #preprocess the mask \n",
    "        # mask[mask >= 0.5] = 1 \n",
    "        # mask[mask < 0.5] = 0\n",
    "\n",
    "        mask2[mask2 >= 0.5] = 1 \n",
    "        mask2[mask2 < 0.5] = 0\n",
    "\n",
    "        rotate = bool(random.getrandbits(1))\n",
    "        if rotate:\n",
    "            mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE)\n",
    "        batch_y.append(mask)\n",
    "\n",
    "        #preprocess the raw images \n",
    "        raw = Image.open(f'{path_dataset}/images/{f}')\n",
    "        h, w = raw.size\n",
    "        raw = np.array(raw)\n",
    "        mask2 = np.stack((mask2,)*3, axis=-1)\n",
    "        \n",
    "        # raw = raw.resize(sz)\n",
    "        raw = cv2.resize(raw, sz, interpolation = cv2.INTER_CUBIC)\n",
    "        raw = np.array(raw)\n",
    "        #print(raw.shape, mask.shape)\n",
    "        if rotate:\n",
    "            raw = cv2.rotate(raw, cv2.ROTATE_90_CLOCKWISE)\n",
    "        #batch_x.append(np.stack((raw,)*3,axis=-1))\n",
    "        batch_x.append(raw)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255.\n",
    "    batch_y = np.array(batch_y)/255.\n",
    "    batch_y = np.expand_dims(batch_y,3)\n",
    "    yield (batch_x, batch_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea8911",
   "metadata": {
    "cellId": "b45p91q455twepgco1vbu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "batch_size = 5\n",
    "epochs = 150\n",
    "all_files = os.listdir(f'{path_dataset}/images')\n",
    "shuffle(all_files)\n",
    "split = int(0.85 * len(all_files))\n",
    "#split into training and testing\n",
    "train_files = all_files[0:split]\n",
    "test_files  = all_files[split:]\n",
    "train_generator = image_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_generator(test_files, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d2a5d",
   "metadata": {
    "cellId": "3s7nmztjxxw344y083qe5b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "n_learn = 3\n",
    "def build_callbacks():\n",
    "    os.makedirs(f'chkpt/Unet/{folder_chkpt}/learn_{n_learn}', exist_ok=True)\n",
    "    checkpointer = ModelCheckpoint(filepath='chkpt/Unet/'+str(folder_chkpt)+'/learn_'+str(n_learn)+'/model_{epoch:02d}loss_{val_loss:.3f}_mi_{val_mean_iou:.2f}.h5', verbose=1, monitor='val_mean_iou', mode=\"max\", save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [checkpointer, PlotLearning()]\n",
    "    return callbacks\n",
    "\n",
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('mean_iou'))\n",
    "        self.val_acc.append(logs.get('val_mean_iou'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        raw = Image.open(f'{path_dataset}/images/{path}')\n",
    "        #raw = np.stack((np.array(raw.resize((256, 256)))/255.,)*3, axis=-1)\n",
    "        raw = np.array(raw.resize((256, 256)))/255\n",
    "        #raw.resize((256, 256))/255\n",
    "        raw = raw[:,:,0:3]\n",
    "        mask_true = Image.open(f'{path_dataset}/masks/{path}')\n",
    "        mask_true = np.array(mask_true.resize((256, 256)))        \n",
    "        #predict the mask \n",
    "        pred = model.predict(np.expand_dims(raw, 0))\n",
    "        \n",
    "        #mask post-processing \n",
    "        msk  = pred.squeeze()\n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "        msk[msk >= 0.5] = 1 \n",
    "        msk[msk < 0.5] = 0 \n",
    "\n",
    "        mask_true_bin = mask_true[..., 0]\n",
    "        msk_bin = msk[..., 0]*255\n",
    "        contours_true, _ = cv2.findContours(mask_true_bin.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours_pred, _ = cv2.findContours(msk_bin.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Создаем копию исходного изображения для рисования\n",
    "        result_img = raw.copy()\n",
    "\n",
    "        # Рисуем контуры (цвета в BGR: красный и зеленый)\n",
    "        result_img = cv2.drawContours(result_img, contours_true, -1, (255, 0, 0), 1)  # красный\n",
    "        result_img = cv2.drawContours(result_img, contours_pred, -1, (0, 0, 255), 1)  # зеленый\n",
    "        plt.imshow(result_img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        #show the mask and the segmented image \n",
    "        # combined = np.concatenate([raw, raw*msk, raw*mask_true], axis = 1)\n",
    "        # plt.axis('off')\n",
    "        # plt.imshow(combined)\n",
    "        # plt.show()\n",
    "        #if self.i == epochs:\n",
    "\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        fig, axs = plt.subplots(2)\n",
    "       \n",
    "        axs[0].plot(self.x, self.acc, marker = 'o', ms=4, label=\"Train\")\n",
    "        axs[0].plot(self.x, self.val_acc, marker = 'o', ms=4, label=\"Validation\")\n",
    "        axs[0].set_xlabel(\"Epochs, Num\")\n",
    "        axs[0].legend()\n",
    "        axs[0].set_ylabel(\"Value Accuracy (Mean IOU), %\")\n",
    "        axs[0].set_xticks(np.arange(min(self.x), max(self.x)+1, 1.0))\n",
    "        axs[0].grid()\n",
    "\n",
    "        axs[1].plot(self.x, self.losses, marker = 'o', ms=4, label=\"Train\")\n",
    "        axs[1].plot(self.x, self.val_losses, marker = 'o', ms=4, label=\"Validation\")\n",
    "        axs[1].set_xlabel(\"Epochs, Num\")\n",
    "        axs[1].legend()\n",
    "        axs[1].set_ylabel(\"Value Loss, %\")\n",
    "        axs[1].set_xticks(np.arange(min(self.x), max(self.x)+1, 1.0))\n",
    "        axs[1].grid()\n",
    "        plt.savefig('chkpt/Unet/'+str(folder_chkpt)+'/learn_'+str(n_learn)+'.png')\n",
    "        import pandas as pd\n",
    "        data = {\n",
    "            'epochs': self.x,\n",
    "            'train_acc': self.acc,\n",
    "            'val_acc': self.val_acc,\n",
    "            'train_loss': self.losses,\n",
    "            'valid_loss': self.val_losses\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('chkpt/Unet/'+str(folder_chkpt)+'/learn_'+str(n_learn)+'.csv')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068d90f",
   "metadata": {
    "cellId": "surikjt5a7jotplzszv57m"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "x, y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877aa2f7",
   "metadata": {
    "cellId": "f1ccxqzgkm10123tcv8in",
    "execution_id": "ce46cb86-ee41-4387-8033-4531e4cfc34f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_steps = len(train_files)//batch_size\n",
    "test_steps = 1\n",
    "model.load_weights(f'chkpt/Unet/{folder_chkpt}/learn_{n_learn-1}/model_final[WITH_UP].h5')\n",
    "model.fit_generator(train_generator, \n",
    "                    epochs = epochs, steps_per_epoch = train_steps, validation_data = test_generator, validation_steps = test_steps,\n",
    "                    callbacks = build_callbacks(), verbose = 0)\n",
    "model.save(f'chkpt/Unet/{folder_chkpt}/learn_{n_learn}/model_final[WITH_UP].h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "notebookId": "1b39695e-5d62-418b-9ca2-9780a4ff6688",
  "notebookPath": "Unet2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
